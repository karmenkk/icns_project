{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sample_length_comparison.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_pnqNTclU0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6e38f9-b3f1-4cb7-9fe3-5a455731eba7"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install plotly==4.14.1\r\n",
        "!pip install datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: plotly==4.14.1 in /usr/local/lib/python3.6/dist-packages (4.14.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.14.1) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.14.1) (1.15.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VplqC2Il2Nc",
        "outputId": "03d5ff66-8abf-49b9-d17f-ba60ef0e62fe"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BYz7j82l6D6"
      },
      "source": [
        "import sys\r\n",
        "sys.path.append('/content/drive/My Drive/data/icns_project')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHbW7dmel0pa"
      },
      "source": [
        "import torch\r\n",
        "import pandas as pd\r\n",
        "import plotly.express as px\r\n",
        "from transformers import RobertaTokenizer, RobertaModel\r\n",
        "from datasets import load_dataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from pathlib import Path\r\n",
        "\r\n",
        "from collections import defaultdict\r\n",
        "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\r\n",
        "from scipy.spatial.distance import euclidean, pdist, squareform\r\n",
        "from sklearn import manifold          #use this for MDS computation\r\n",
        "\r\n",
        "#visualization libs\r\n",
        "import plotly.graph_objects as go\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "% matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCgCVRQ-QTto"
      },
      "source": [
        "pd.set_option('max_colwidth', 800)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3QxopnuxzEB",
        "outputId": "03f6916d-9958-47d4-84a6-c8194e52b999"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print('using device: ', torch.cuda.get_device_name(device), flush=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using device:  Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkf3PvQisU6T"
      },
      "source": [
        "# Load pre-trained model tokenizer\r\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhE709S6wXAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95102f25-bc2e-4fb8-f7a6-af9a5d60bf24"
      },
      "source": [
        "# Load pre-trained model\r\n",
        "model = RobertaModel.from_pretrained('roberta-base',\r\n",
        "                                  output_hidden_states = True\r\n",
        "                                  )\r\n",
        "model.to(device)\r\n",
        "# Put the model in \"evaluation\" mode\r\n",
        "model.eval()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p8JxGdCmKhN"
      },
      "source": [
        "MODEL_PATH = Path('drive') / 'My Drive' / 'data' / 'icns_project' / 'paraphrase-distilroberta-base-v1'\r\n",
        "DATA_PATH = Path('drive') / 'My Drive' / 'data' / 'icns_project'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGJIkSs7snR7"
      },
      "source": [
        "news_df = pd.read_csv(DATA_PATH / 'BBC_news_adjusted.csv', encoding='utf-8')\r\n",
        "jokes_df = pd.read_csv(DATA_PATH / 'jokes_stupid_wocka.csv', encoding='utf-8')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "SbvEWPSxtAqh",
        "outputId": "9758ca35-a313-4b05-bbe3-cf863354590c"
      },
      "source": [
        "news_df.head(2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness. cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002.</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>german business confidence slides german business confidence fell in february knocking hopes of a speedy recovery in europe s largest economy. munich-based research institute ifo said that its confidence index fell to 95.5 in february from 97.5 in january  its first decline in three months.</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                      Text  Category\n",
              "0  worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness. cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002.  business\n",
              "1                                      german business confidence slides german business confidence fell in february knocking hopes of a speedy recovery in europe s largest economy. munich-based research institute ifo said that its confidence index fell to 95.5 in february from 97.5 in january  its first decline in three months.  business"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "f_RfmDVETZ5_",
        "outputId": "ff90ef1c-4753-4627-b15f-f7c8971001ed"
      },
      "source": [
        "jokes_df.head(2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>source</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A blackjack dealer and a player with a thirteen count in his hand were arguing about whether or not it was appropriate to tip the dealer.  The player said, \"When I get bad cards, it's not the dealer's fault. Accordingly, when I get good cards, the dealer obviously had nothing to do with it so, why should I tip him?\"  The dealer said, \"When you eat out do you tip the waiter?\"  \"Yes.\"  \"Well then, he serves you food, I'm serving you cards, so you should tip me.\"  \"Okay, but, the waiter gives me what I ask for. I'll take an eight.\"</td>\n",
              "      <td>Children</td>\n",
              "      <td>stupidstuff</td>\n",
              "      <td>2.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>At a dinner party, several of the guests were arguing whether men or women were more trustworthy. 'No woman,' said one man, scornfully, 'can keep a secret.' 'I don't know about that,' answered a blonde woman guest. 'I have kept my age a secret since I was twenty-one.' 'You'll let it out some day,' the man insisted. 'I hardly think so!' responded the blonde lady. 'When a woman has kept a secret for twenty-seven years, she can keep it forever.'</td>\n",
              "      <td>Blonde Jokes</td>\n",
              "      <td>stupidstuff</td>\n",
              "      <td>2.57</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text  ... score\n",
              "0  A blackjack dealer and a player with a thirteen count in his hand were arguing about whether or not it was appropriate to tip the dealer.  The player said, \"When I get bad cards, it's not the dealer's fault. Accordingly, when I get good cards, the dealer obviously had nothing to do with it so, why should I tip him?\"  The dealer said, \"When you eat out do you tip the waiter?\"  \"Yes.\"  \"Well then, he serves you food, I'm serving you cards, so you should tip me.\"  \"Okay, but, the waiter gives me what I ask for. I'll take an eight.\"  ...  2.63\n",
              "1                                                                                          At a dinner party, several of the guests were arguing whether men or women were more trustworthy. 'No woman,' said one man, scornfully, 'can keep a secret.' 'I don't know about that,' answered a blonde woman guest. 'I have kept my age a secret since I was twenty-one.' 'You'll let it out some day,' the man insisted. 'I hardly think so!' responded the blonde lady. 'When a woman has kept a secret for twenty-seven years, she can keep it forever.'  ...  2.57\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0F0dtacnZ4H"
      },
      "source": [
        "news_df = news_df.rename({'Text': 'text'}, axis=1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19U-_zuzvYWv"
      },
      "source": [
        "na_rows = news_df[news_df['text'].isna()]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "d76j79EGV2NC",
        "outputId": "b2b03164-855c-4467-b968-54d8db59a916"
      },
      "source": [
        "na_rows"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text, Category]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfwDwEEoV3wV"
      },
      "source": [
        "na_rows = jokes_df[jokes_df['text'].isna()]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "awKrrnyTV6u_",
        "outputId": "281fe4b2-0fa4-4a3f-c11e-f73cca5366a3"
      },
      "source": [
        "na_rows"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>source</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3281</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Other / Misc</td>\n",
              "      <td>wocka</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     text      category source  score\n",
              "3281  NaN  Other / Misc  wocka    NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kpBwiyFvkCp"
      },
      "source": [
        "jokes_df = jokes_df.drop(na_rows.index)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTEjmgiZUdMH",
        "outputId": "46e5b160-cb6b-4f1d-af99-c5acf834179c"
      },
      "source": [
        "news_df.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1490, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR1QpdftV_65",
        "outputId": "50518a60-87ad-492f-d522-1933a1376119"
      },
      "source": [
        "jokes_df.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13132, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_RqGfFGWOgd"
      },
      "source": [
        "# let's look at the lengths of jokes in (possibly) relevant categories only\r\n",
        "categories = ['Sports', 'Business', 'Tech', 'News / Politics', 'Political',\r\n",
        "              'Money', 'At Work', 'Office Jokes', 'Computers']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ_NN77zXoxB"
      },
      "source": [
        "jokes_df = jokes_df[jokes_df['category'].isin(categories)]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWmlLfpBXzQd",
        "outputId": "c21a6641-30cf-421e-846a-023ec81e71ab"
      },
      "source": [
        "jokes_df.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1201, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0yVJJ5ytCko",
        "outputId": "cb533305-4016-4da0-8555-735f5f2d2429"
      },
      "source": [
        "%%time\r\n",
        "# tokenize without padding and truncation\r\n",
        "news_encodings = tokenizer(news_df['text'].to_list())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 870 ms, sys: 37 ms, total: 907 ms\n",
            "Wall time: 920 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CEyCzi1WFYo",
        "outputId": "e17fd5f4-2944-4c63-c766-d8f017149fe3"
      },
      "source": [
        "%%time\r\n",
        "# tokenize without padding and truncation\r\n",
        "jokes_encodings = tokenizer(jokes_df['text'].to_list())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (595 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.55 s, sys: 4.89 ms, total: 1.56 s\n",
            "Wall time: 1.56 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q1xsRe8uodb"
      },
      "source": [
        "news_lens = [len(ex) for ex in news_encodings['input_ids']]\r\n",
        "jokes_lens = [len(ex) for ex in jokes_encodings['input_ids']]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn3r3vt8wHlY"
      },
      "source": [
        "news_lens_series = pd.Series(news_lens, name='number_of_tokens')\r\n",
        "jokes_lens_series = pd.Series(jokes_lens, name='number_of_tokens')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn3mMMVQyJvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd461d27-bbd4-4cb9-fd09-aa552dee912c"
      },
      "source": [
        "news_lens_series.describe()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1490.000000\n",
              "mean       66.089933\n",
              "std        10.718886\n",
              "min        20.000000\n",
              "25%        59.000000\n",
              "50%        66.000000\n",
              "75%        72.000000\n",
              "max       114.000000\n",
              "Name: number_of_tokens, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMggIhRPYEfj",
        "outputId": "2d8d24a0-f424-48dc-d45a-bf2d978ec1c3"
      },
      "source": [
        "jokes_lens_series.describe()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     1201.000000\n",
              "mean       241.198168\n",
              "std        569.486391\n",
              "min          5.000000\n",
              "25%         76.000000\n",
              "50%        158.000000\n",
              "75%        272.000000\n",
              "max      17171.000000\n",
              "Name: number_of_tokens, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke5DUfT3acqj",
        "outputId": "934a5f16-cc5b-4f7a-df30-7e703d1eb57a"
      },
      "source": [
        "jokes_lens_series[jokes_lens_series > 512].shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYZzgATgg6Av"
      },
      "source": [
        "#set index from 0 to n\r\n",
        "jokes_df = jokes_df.reset_index(drop=True)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFulKsFPgh3H"
      },
      "source": [
        "news_df['length'] = news_lens_series\r\n",
        "jokes_df['length'] = jokes_lens_series"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "NfyrDN8Cgo93",
        "outputId": "bbb7eb1d-33dc-4ec9-9815-233dd308917c"
      },
      "source": [
        "jokes_df.head(3)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>source</th>\n",
              "      <th>score</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A brunette, a blonde, and a redhead all worked in the same office with the same female boss. Every day, they noticed their boss left work early.  One day, the girls decided that when the boss left, they'd leave right behind her. After all, she never called in or came back to the office when she left early, so how was she to know?  The next day, they all three left the office right after the boss left. The brunette was thrilled to be home early. She did a little gardening and went to bed early.  The redhead was elated to be able to get in a quick workout at the health club before meeting her dinner date.  The blonde was happy, happy, happy to be home, but when she got to the bedroom she heard a muffled noise from inside. Slowly, quietly, she cracked open the door and was mortified to se...</td>\n",
              "      <td>Office Jokes</td>\n",
              "      <td>stupidstuff</td>\n",
              "      <td>3.73</td>\n",
              "      <td>260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bill and Hillary Clinton went out to dinner and when the waiter came to take their order, he asked Bill how he wanted his steak, she replied, \"medium.\"  Then the waiter said, \"how about your vegetable?\" Bill replied, \"Oh, she can order for herself.\"</td>\n",
              "      <td>Political</td>\n",
              "      <td>stupidstuff</td>\n",
              "      <td>3.50</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The stockbroker's secretary answered his phone one morning. \"I'm sorry,\" she said, \"Mr. Bradford's on another line.\"  \"This is Mr. Ingram's office,\" the caller said. \"We'd like to know if he's bullish or bearish right now.\"  \"He's talking to his wife,\" the secretary replied. \"Right now I'd say he's sheepish.\"</td>\n",
              "      <td>Business</td>\n",
              "      <td>stupidstuff</td>\n",
              "      <td>2.33</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  ... length\n",
              "0  A brunette, a blonde, and a redhead all worked in the same office with the same female boss. Every day, they noticed their boss left work early.  One day, the girls decided that when the boss left, they'd leave right behind her. After all, she never called in or came back to the office when she left early, so how was she to know?  The next day, they all three left the office right after the boss left. The brunette was thrilled to be home early. She did a little gardening and went to bed early.  The redhead was elated to be able to get in a quick workout at the health club before meeting her dinner date.  The blonde was happy, happy, happy to be home, but when she got to the bedroom she heard a muffled noise from inside. Slowly, quietly, she cracked open the door and was mortified to se...  ...    260\n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Bill and Hillary Clinton went out to dinner and when the waiter came to take their order, he asked Bill how he wanted his steak, she replied, \"medium.\"  Then the waiter said, \"how about your vegetable?\" Bill replied, \"Oh, she can order for herself.\"  ...     59\n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The stockbroker's secretary answered his phone one morning. \"I'm sorry,\" she said, \"Mr. Bradford's on another line.\"  \"This is Mr. Ingram's office,\" the caller said. \"We'd like to know if he's bullish or bearish right now.\"  \"He's talking to his wife,\" the secretary replied. \"Right now I'd say he's sheepish.\"  ...     85\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvEQRxEciBH4",
        "outputId": "27de77b2-9151-44cb-d793-24d323a28687"
      },
      "source": [
        "# how many of the too long jokes come from stupidstuff vs wocka?\r\n",
        "long_jokes = jokes_df[jokes_df['length'] > 512]\r\n",
        "long_jokes['source'].value_counts()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "wocka          74\n",
              "stupidstuff    24\n",
              "Name: source, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSQYP8tRiOX0"
      },
      "source": [
        "# remove too long jokes\r\n",
        "jokes_df = jokes_df.drop(long_jokes.index)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDz2OewlicoD",
        "outputId": "4677276f-1171-4ab8-8407-4d8ab85a1fca"
      },
      "source": [
        "jokes_df.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1103, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KQmGGJEnojb",
        "outputId": "67e73cab-f78c-4d56-e70f-2bf243842c75"
      },
      "source": [
        "! pip install -U kaleido"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: kaleido in /usr/local/lib/python3.6/dist-packages (0.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "omMI-beoxa2e",
        "outputId": "a8414251-c0dd-4977-d80e-5d6b0c36ad71"
      },
      "source": [
        "fig = go.Figure()\r\n",
        "fig.add_trace(go.Histogram(x=news_df['length'], name='news',\r\n",
        "                          xbins=dict( # bins used for histogram\r\n",
        "                            start=0,\r\n",
        "                            end=512,\r\n",
        "                            size=50\r\n",
        "                            ),\r\n",
        "                           marker_color='#EB89B5',\r\n",
        "                          )\r\n",
        ")\r\n",
        "fig.add_trace(go.Histogram(x=jokes_df['length'], name='jokes',\r\n",
        "                           xbins=dict( # bins used for histogram\r\n",
        "                              start=0,\r\n",
        "                              end=512,\r\n",
        "                              size=50\r\n",
        "                              ),\r\n",
        "                           marker_color='#330C73',\r\n",
        "                          )\r\n",
        ")\r\n",
        "# Overlay both histograms\r\n",
        "fig.update_layout(\r\n",
        "    #barmode='overlay',\r\n",
        "                  title='Sample lengths (number of tokens)',\r\n",
        "                  xaxis_title='Number of tokens',\r\n",
        "                  yaxis_title='Frequency',\r\n",
        "                  legend_title='Source',\r\n",
        "                  bargap=0.2, # gap between bars of adjacent location coordinates\r\n",
        "                  )\r\n",
        "# Reduce opacity to see both histograms\r\n",
        "fig.update_traces(opacity=0.7)\r\n",
        "fig.show()\r\n",
        "fig.write_image(str(DATA_PATH / 'sample_length_histogram.png'))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>                <div id=\"edf15eb8-d575-424f-b700-b45df020f88b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"edf15eb8-d575-424f-b700-b45df020f88b\")) {                    Plotly.newPlot(                        \"edf15eb8-d575-424f-b700-b45df020f88b\",                        [{\"marker\": {\"color\": \"#EB89B5\"}, \"name\": \"news\", \"opacity\": 0.7, \"type\": \"histogram\", \"x\": [65, 66, 40, 56, 70, 72, 93, 69, 66, 60, 55, 64, 67, 69, 80, 58, 60, 90, 65, 54, 64, 69, 60, 63, 49, 99, 82, 90, 64, 72, 60, 80, 58, 64, 68, 68, 59, 67, 58, 45, 62, 78, 68, 82, 71, 62, 73, 51, 53, 68, 59, 79, 60, 73, 63, 58, 58, 57, 43, 52, 63, 76, 70, 60, 68, 95, 46, 52, 51, 69, 65, 59, 61, 58, 72, 64, 71, 71, 56, 58, 46, 66, 61, 72, 70, 59, 69, 66, 61, 63, 63, 74, 61, 58, 56, 63, 59, 48, 94, 65, 77, 70, 65, 54, 40, 52, 20, 65, 62, 58, 55, 66, 83, 77, 62, 74, 64, 64, 63, 73, 60, 75, 76, 44, 62, 56, 51, 57, 64, 71, 97, 52, 61, 57, 56, 71, 70, 61, 59, 78, 50, 76, 66, 72, 51, 48, 54, 80, 88, 93, 63, 61, 62, 87, 65, 86, 59, 70, 52, 69, 64, 61, 50, 73, 57, 59, 70, 103, 66, 63, 66, 57, 66, 60, 71, 68, 52, 57, 73, 57, 78, 64, 64, 61, 63, 69, 51, 76, 77, 64, 59, 55, 78, 56, 75, 66, 62, 57, 64, 74, 73, 70, 72, 83, 39, 64, 78, 90, 61, 73, 60, 63, 65, 56, 67, 61, 68, 71, 80, 63, 77, 72, 63, 87, 66, 74, 49, 61, 63, 47, 78, 79, 66, 77, 65, 67, 69, 65, 77, 67, 65, 60, 74, 81, 65, 66, 74, 67, 68, 63, 57, 68, 68, 62, 58, 55, 68, 58, 64, 60, 61, 89, 59, 56, 89, 69, 67, 61, 55, 60, 67, 71, 71, 63, 75, 81, 60, 48, 73, 56, 61, 67, 78, 76, 65, 103, 67, 59, 63, 74, 65, 78, 68, 94, 65, 32, 55, 82, 66, 59, 61, 54, 69, 70, 72, 66, 72, 69, 76, 59, 60, 56, 71, 67, 51, 65, 66, 58, 85, 72, 60, 68, 61, 62, 94, 67, 75, 49, 63, 102, 54, 74, 68, 65, 67, 66, 65, 69, 70, 46, 66, 60, 87, 69, 73, 67, 62, 77, 57, 55, 61, 61, 81, 32, 62, 52, 59, 54, 75, 72, 69, 72, 58, 61, 102, 58, 68, 72, 50, 52, 72, 95, 112, 57, 57, 61, 61, 77, 62, 90, 59, 80, 73, 42, 52, 64, 66, 78, 61, 64, 79, 72, 65, 68, 73, 78, 99, 58, 74, 66, 63, 67, 61, 76, 68, 50, 71, 66, 70, 65, 58, 80, 53, 74, 73, 77, 70, 65, 67, 63, 71, 58, 68, 67, 64, 60, 56, 63, 43, 68, 70, 63, 77, 67, 58, 56, 74, 70, 73, 65, 56, 46, 59, 68, 66, 63, 67, 56, 79, 54, 68, 46, 67, 65, 87, 47, 66, 52, 75, 58, 76, 71, 66, 33, 52, 60, 64, 76, 65, 66, 82, 65, 74, 80, 66, 58, 76, 52, 82, 64, 71, 58, 78, 68, 58, 75, 78, 86, 58, 70, 56, 75, 63, 48, 63, 74, 67, 71, 97, 62, 66, 70, 63, 43, 66, 67, 96, 60, 65, 56, 66, 51, 70, 62, 52, 79, 71, 97, 59, 59, 76, 66, 74, 52, 87, 71, 58, 55, 56, 52, 70, 69, 71, 55, 78, 58, 77, 70, 61, 80, 70, 66, 68, 51, 77, 69, 71, 77, 56, 65, 64, 56, 89, 63, 48, 53, 66, 66, 54, 57, 63, 53, 75, 61, 67, 60, 68, 69, 67, 55, 68, 63, 69, 63, 50, 70, 61, 67, 74, 70, 64, 75, 50, 85, 61, 70, 69, 58, 73, 60, 69, 67, 71, 53, 69, 62, 54, 64, 58, 114, 64, 66, 57, 55, 66, 74, 54, 59, 73, 79, 77, 69, 61, 69, 54, 61, 46, 74, 63, 61, 62, 62, 69, 75, 72, 70, 67, 64, 59, 67, 69, 78, 69, 59, 88, 102, 61, 71, 61, 60, 76, 67, 70, 55, 65, 63, 79, 69, 77, 77, 64, 55, 65, 61, 61, 67, 66, 65, 67, 55, 78, 68, 69, 61, 51, 71, 64, 58, 86, 62, 61, 65, 65, 63, 54, 66, 68, 72, 67, 57, 80, 64, 65, 65, 62, 58, 70, 61, 58, 67, 63, 59, 68, 60, 42, 77, 48, 59, 67, 54, 66, 66, 63, 61, 72, 68, 65, 75, 64, 63, 68, 86, 56, 65, 55, 69, 69, 61, 76, 57, 50, 96, 72, 63, 81, 62, 68, 83, 62, 77, 75, 64, 64, 64, 61, 65, 68, 59, 58, 68, 74, 62, 74, 81, 68, 51, 72, 63, 76, 59, 63, 61, 84, 77, 102, 61, 70, 59, 61, 55, 48, 54, 82, 77, 71, 51, 67, 54, 58, 52, 69, 58, 65, 84, 56, 68, 57, 53, 47, 61, 58, 65, 58, 46, 60, 61, 72, 66, 61, 68, 87, 68, 71, 78, 66, 67, 78, 59, 84, 76, 67, 60, 65, 73, 76, 73, 51, 71, 56, 61, 75, 48, 73, 75, 81, 80, 76, 43, 64, 43, 66, 59, 63, 76, 80, 55, 66, 63, 69, 51, 92, 59, 72, 71, 62, 55, 58, 70, 88, 65, 55, 52, 65, 59, 49, 67, 79, 69, 74, 62, 60, 69, 55, 87, 53, 62, 79, 68, 58, 86, 60, 55, 79, 63, 54, 72, 68, 59, 71, 51, 67, 89, 68, 68, 71, 71, 79, 66, 58, 69, 69, 78, 68, 56, 72, 51, 66, 70, 44, 56, 51, 74, 59, 69, 64, 45, 74, 69, 54, 64, 63, 65, 92, 58, 89, 66, 74, 69, 55, 80, 49, 65, 47, 65, 60, 68, 74, 77, 70, 64, 46, 85, 71, 56, 72, 67, 79, 58, 50, 60, 70, 68, 57, 69, 108, 75, 72, 68, 86, 55, 55, 67, 55, 55, 80, 42, 68, 62, 59, 58, 75, 72, 86, 55, 67, 63, 88, 44, 47, 54, 61, 74, 51, 81, 79, 65, 52, 75, 56, 58, 70, 63, 73, 65, 80, 71, 63, 65, 46, 69, 72, 55, 62, 79, 58, 46, 40, 82, 70, 69, 55, 66, 68, 69, 78, 75, 55, 70, 78, 63, 67, 67, 65, 69, 68, 62, 67, 50, 64, 67, 65, 62, 65, 58, 69, 65, 63, 49, 61, 63, 57, 61, 56, 73, 84, 79, 67, 67, 57, 57, 60, 55, 62, 72, 78, 51, 79, 75, 61, 77, 71, 74, 86, 55, 73, 77, 65, 56, 68, 71, 72, 65, 84, 65, 66, 84, 66, 65, 65, 103, 63, 63, 52, 77, 54, 55, 79, 58, 114, 58, 80, 66, 55, 50, 53, 62, 60, 51, 74, 64, 39, 63, 68, 68, 75, 73, 53, 67, 63, 73, 68, 65, 68, 69, 69, 55, 61, 77, 55, 66, 56, 59, 70, 67, 49, 65, 63, 67, 71, 52, 74, 67, 56, 71, 42, 65, 72, 53, 72, 45, 82, 67, 59, 70, 75, 59, 70, 69, 78, 64, 56, 48, 88, 67, 56, 52, 67, 66, 78, 76, 64, 53, 58, 87, 64, 72, 62, 81, 62, 80, 67, 70, 60, 84, 53, 77, 44, 67, 51, 78, 77, 76, 62, 57, 73, 79, 86, 69, 69, 80, 65, 55, 93, 67, 62, 76, 67, 60, 61, 56, 67, 72, 50, 76, 58, 61, 68, 53, 56, 61, 64, 74, 76, 57, 68, 62, 66, 74, 72, 81, 73, 50, 68, 37, 64, 63, 49, 69, 58, 60, 65, 76, 58, 82, 64, 58, 55, 70, 67, 59, 66, 56, 63, 73, 61, 78, 62, 74, 81, 82, 86, 68, 48, 59, 75, 70, 70, 69, 95, 41, 73, 57, 80, 82, 67, 60, 79, 56, 75, 65, 64, 72, 60, 68, 62, 75, 80, 53, 84, 69, 87, 76, 60, 74, 73, 60, 66, 82, 55, 75, 67, 92, 75, 68, 74, 72, 65, 68, 66, 62, 64, 60, 74, 55, 69, 68, 60, 56, 74, 58, 80, 66, 65, 67, 71, 62, 55, 64, 63, 74, 58, 55, 46, 73, 66, 56, 71, 67, 46, 58, 69, 67, 67, 88, 64, 82, 67, 68, 73, 41, 46, 63, 61, 70, 64, 83, 58, 70, 61, 65, 58, 74, 77, 83, 61, 60, 54, 69, 61, 63, 59, 70, 68, 69, 75, 60, 52, 77, 56, 73, 63, 55, 51, 82, 69, 72, 64, 63, 85, 82, 53, 70, 87, 49, 68, 67, 70, 58, 53, 75, 52, 64, 64, 75, 75, 62, 67, 56, 67, 93, 69, 82, 76, 54, 70, 61, 62, 61, 54, 70, 65, 69, 80, 64, 64, 73, 67, 76, 62, 76, 86, 68, 59, 72, 65, 69, 80, 66, 67, 46, 67, 65, 66, 73, 54, 55, 62, 53, 54, 70, 58, 72, 59, 69, 70, 65, 63, 63, 47, 73, 77, 62, 71, 66, 81, 88, 60, 74, 68, 60, 58, 56, 64, 76, 66, 77, 65, 46, 72, 58, 76, 68, 66, 60, 67, 70, 62, 80, 68, 103, 61, 58, 71, 76, 62, 48, 71, 57, 99, 60, 65, 65, 61, 72, 79, 61, 91, 59, 58, 72, 68, 68, 65, 60, 63, 74, 73, 69, 56, 52], \"xbins\": {\"end\": 512, \"size\": 50, \"start\": 0}}, {\"marker\": {\"color\": \"#330C73\"}, \"name\": \"jokes\", \"opacity\": 0.7, \"type\": \"histogram\", \"x\": [260, 59, 85, 91, 95, 228, 67, 106, 120, 381, 236, 61, 234, 340, 258, 327, 84, 156, 167, 353, 171, 206, 156, 337, 118, 385, 347, 422, 108, 98, 209, 485, 189, 193, 297, 280, 335, 303, 313, 332, 244, 218, 382, 419, 173, 199, 315, 363, 271, 239, 186, 311, 260, 259, 111, 330, 186, 312, 181, 202, 117, 338, 455, 143, 86, 216, 271, 360, 208, 152, 187, 215, 147, 416, 214, 90, 176, 382, 146, 290, 182, 230, 127, 124, 138, 213, 371, 298, 44, 116, 279, 152, 280, 153, 227, 135, 56, 181, 170, 299, 153, 186, 198, 126, 142, 215, 191, 209, 126, 259, 23, 245, 182, 218, 303, 122, 88, 153, 206, 29, 167, 188, 180, 126, 210, 168, 79, 256, 112, 67, 64, 331, 205, 142, 292, 401, 202, 220, 64, 124, 248, 204, 147, 217, 68, 352, 198, 301, 159, 110, 32, 23, 21, 161, 74, 124, 312, 231, 256, 221, 185, 224, 125, 95, 123, 92, 38, 186, 462, 269, 427, 282, 201, 183, 88, 83, 195, 471, 125, 170, 66, 105, 164, 405, 292, 251, 96, 197, 187, 202, 124, 267, 80, 54, 160, 166, 100, 156, 56, 169, 296, 159, 252, 398, 280, 92, 42, 42, 145, 161, 271, 166, 276, 494, 343, 258, 15, 248, 93, 331, 176, 103, 18, 496, 243, 452, 286, 176, 25, 225, 138, 46, 152, 45, 45, 32, 145, 86, 145, 78, 217, 114, 457, 140, 322, 215, 126, 119, 220, 49, 53, 25, 421, 156, 121, 188, 264, 450, 67, 44, 25, 496, 16, 67, 178, 21, 53, 123, 15, 240, 23, 22, 272, 97, 366, 392, 375, 263, 41, 109, 54, 497, 329, 226, 117, 10, 469, 255, 458, 294, 277, 195, 29, 37, 186, 52, 187, 213, 193, 24, 26, 32, 109, 190, 301, 318, 5, 415, 344, 139, 281, 421, 13, 348, 114, 54, 75, 58, 350, 328, 34, 25, 18, 503, 198, 44, 360, 187, 247, 319, 246, 167, 100, 200, 169, 148, 84, 258, 227, 23, 243, 344, 346, 231, 330, 97, 316, 32, 304, 180, 339, 163, 372, 43, 271, 173, 17, 207, 160, 212, 145, 164, 113, 81, 212, 293, 497, 28, 105, 191, 380, 105, 140, 19, 74, 144, 38, 508, 86, 55, 23, 30, 115, 165, 169, 179, 132, 397, 486, 206, 158, 185, 93, 109, 307, 248, 350, 243, 135, 129, 375, 405, 100, 126, 80, 230, 402, 297, 224, 68, 258, 269, 256, 142, 305, 306, 130, 290, 111, 41, 223, 94, 139, 268, 22, 144, 91, 319, 157, 60, 144, 448, 211, 34, 78, 67, 109, 44, 169, 201, 114, 101, 311, 178, 85, 54, 23, 448, 273, 236, 410, 108, 152, 17, 89, 230, 22, 30, 27, 181, 183, 144, 131, 425, 296, 178, 119, 260, 21, 141, 46, 124, 129, 218, 391, 223, 313, 243, 226, 250, 163, 392, 472, 134, 384, 52, 208, 80, 121, 210, 200, 109, 172, 261, 347, 108, 82, 196, 91, 97, 362, 122, 83, 254, 275, 250, 20, 36, 24, 54, 22, 25, 28, 49, 137, 121, 248, 207, 298, 121, 216, 106, 332, 260, 405, 115, 22, 283, 84, 49, 156, 48, 117, 303, 72, 176, 145, 334, 94, 36, 487, 231, 182, 52, 157, 141, 195, 28, 169, 38, 187, 164, 277, 91, 185, 55, 136, 256, 245, 236, 195, 16, 195, 22, 45, 313, 36, 252, 454, 141, 216, 333, 178, 298, 73, 215, 138, 143, 307, 258, 26, 152, 284, 182, 203, 175, 94, 76, 126, 105, 187, 33, 44, 195, 22, 308, 394, 39, 79, 20, 273, 33, 187, 49, 181, 233, 204, 364, 158, 67, 117, 45, 97, 355, 278, 281, 169, 44, 17, 67, 138, 91, 26, 58, 155, 41, 127, 21, 27, 151, 25, 114, 201, 50, 135, 76, 39, 18, 31, 99, 223, 255, 10, 282, 74, 297, 221, 88, 282, 274, 222, 156, 187, 264, 219, 72, 208, 281, 200, 110, 135, 240, 20, 62, 25, 496, 23, 27, 30, 103, 80, 125, 79, 308, 19, 48, 61, 40, 63, 147, 124, 95, 126, 157, 210, 105, 102, 173, 23, 32, 198, 78, 86, 219, 24, 313, 96, 20, 429, 58, 128, 121, 108, 257, 96, 28, 7, 73, 385, 91, 66, 134, 338, 8, 257, 98, 37, 303, 197, 19, 22, 272, 34, 91, 174, 77, 80, 123, 173, 215, 109, 86, 37, 334, 460, 184, 239, 199, 210, 148, 53, 140, 39, 19, 119, 112, 206, 125, 476, 75, 67, 237, 94, 104, 124, 147, 218, 39, 36, 232, 192, 213, 224, 387, 222, 94, 46, 209, 250, 206, 62, 56, 58, 150, 246, 218, 76, 64, 90, 310, 32, 75, 116, 384, 131, 267, 355, 132, 276, 245, 250, 210, 202, 238, 62, 161, 73, 251, 102, 92, 76, 87, 216, 34, 238, 54, 380, 462, 213, 363, 395, 338, 376, 37, 143, 96, 38, 371, 368, 307, 151, 450, 102, 91, 34, 355, 32, 119, 51, 231, 51, 348, 40, 63, 102, 27, 14, 44, 145, 190, 92, 51, 55, 150, 170, 101, 31, 58, 24, 162, 72, 134, 131, 58, 173, 223, 40, 79, 220, 359, 131, 38, 95, 33, 316, 108, 114, 166, 241, 76, 246, 245, 95, 156, 409, 104, 70, 257, 126, 68, 115, 147, 86, 79, 45, 51, 47, 376, 50, 90, 180, 303, 500, 395, 152, 132, 315, 258, 175, 74, 125, 83, 92, 76, 102, 406, 298, 179, 84, 442, 201, 184, 228, 95, 312, 48, 77, 245, 284, 274, 267, 165, 269, 12, 96, 262, 167, 69, 130, 303, 29, 29, 34, 18, 27, 88, 144, 38, 479, 50, 128, 73, 148, 144, 247, 59, 16, 232, 31, 41, 43, 287, 143, 355, 67, 78, 343, 20, 18, 164, 40, 161, 12, 151, 71, 97, 21, 80, 125, 154, 60, 147, 252, 38, 240, 181, 173, 473, 40, 42, 488, 298, 174, 26, 381, 66, 82, 92, 169, 160, 96, 283, 32, 100, 34, 38, 70, 91, 39, 55, 44, 49, 164, 100, 182, 48, 115, 56, 39, 58, 97, 106, 98, 21, 67, 67, 36, 66, 131, 59, 11, 19, 74, 26, 33, 41, 39, 28, 101, 68, 58, 96, 39, 168, 74, 86, 78, 110, 40, 32, 88, 29, 27, 33, 88, 42, 30, 140, 254, 91, 36, 85, 46, 64, 42, 72, 90, 47, 46, 69, 11, 19, 51, 29, 245, 173, 158, 202, 190, 44, 24, 37, 166, 179, 54, 114, 60, 15, 291, 346, 48, 85, 21, 30, 37, 22, 35, 124, 20, 33, 259, 124, 15, 81, 90, 30, 447, 141, 220, 453, 204, 206, 347], \"xbins\": {\"end\": 512, \"size\": 50, \"start\": 0}}],                        {\"bargap\": 0.2, \"legend\": {\"title\": {\"text\": \"Source\"}}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Sample lengths (number of tokens)\"}, \"xaxis\": {\"title\": {\"text\": \"Number of tokens\"}}, \"yaxis\": {\"title\": {\"text\": \"Frequency\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('edf15eb8-d575-424f-b700-b45df020f88b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d3hOSkXpfGK",
        "outputId": "fa766836-3be1-47d2-aff7-746874754aa1"
      },
      "source": [
        "jokes_df['category'].value_counts()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "At Work            264\n",
              "News / Politics    258\n",
              "Sports             166\n",
              "Tech               126\n",
              "Political          109\n",
              "Money               68\n",
              "Computers           55\n",
              "Business            40\n",
              "Office Jokes        17\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aEjuUXGo9Wz"
      },
      "source": [
        "# save jokes dataset with only relevant categories and samples within allowed length boundaries\r\n",
        "jokes_df.to_csv(str(DATA_PATH / 'jokes_stupid_wocka_relevant.csv'), index=False, encoding='utf-8')"
      ],
      "execution_count": 46,
      "outputs": []
    }
  ]
}